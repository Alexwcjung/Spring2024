{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNINV69MvFtzjPOAIYyBBBN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MK316/Spring2024/blob/main/Corpus/Words_in_context.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üçÉ Words in context\n",
        "\n",
        "+ Analyzing words in context is fundamental for accurately interpreting and understanding language, whether in human communication, language learning, or computational language processing."
      ],
      "metadata": {
        "id": "BC4851Myyn8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key methods"
      ],
      "metadata": {
        "id": "lo9JNoMwz_Lb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Tokenization\n",
        "+ Part-of-Speech (POS) Tagging\n",
        "+ Contextual Word Meaning (Word Sense Disambiguation)\n",
        "+ Bi-gram, N-gram, Concordance view\n",
        "+ Collocations\n",
        "+ Sentiment analysis"
      ],
      "metadata": {
        "id": "mYQFYbPP0C--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## {nltk} installation"
      ],
      "metadata": {
        "id": "B9mFGmLr0h9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "qGi1OEvQ0hox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [1] Tokenization\n",
        "\n",
        "+ Purpose: Breaking down text into individual words (tokens) is the first step in many NLP tasks.\n",
        "+ Method: Use nltk.word_tokenize() for tokenizing sentences into words."
      ],
      "metadata": {
        "id": "WJjs8pWk1EMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The quick brown fox jumps over the lazy dog\""
      ],
      "metadata": {
        "id": "kzln7GQ50uf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtMBTSz_yi17"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "QSCmaGF30-mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [2] Part-of-Speech (POS) Tagging\n",
        "\n",
        "+ Purpose: Assigning parts of speech to each word (like noun, verb, adjective) helps in understanding the grammatical context.\n",
        "+ Method: Use nltk.pos_tag().\n",
        "+ [Penn Treebank tagset](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)"
      ],
      "metadata": {
        "id": "mUv_Z0T91ANX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "2fY4uJTy1dtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "pos_tags = pos_tag(tokens)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "id": "5IXJE9j_1SRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [3] Contextual Word Meaning (Word Sense Disambiguation):\n",
        "\n",
        "+ Purpose: Determining the meaning of a word based on the context it appears in.\n",
        "+ Method: Use algorithms like Lesk Algorithm implemented in NLTK.\n",
        "\n",
        "Note: NLTK uses [WordNet](https://wordnet.princeton.edu)"
      ],
      "metadata": {
        "id": "quRKdtmQ2HA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Bank (Meaning 1 - Financial Institution):\n",
        "\n",
        "  + Sentence 1: I need to visit the bank to withdraw some money.\n",
        "  + Sentence 2: The bank of the river was a peaceful place to relax.\n",
        "+ Bat (Meaning 1 - Nocturnal Flying Mammal):\n",
        "\n",
        "  + Sentence 1: I saw a bat flying in the night sky.\n",
        "  + Sentence 2: She used a baseball bat to hit the ball out of the park.\n",
        "+ Book (Meaning 1 - Written or Printed Work):\n",
        "\n",
        "  + Sentence 1: I'm reading a fascinating book about space exploration.\n",
        "  + Sentence 2: Please book a table for two at the restaurant for tonight.\n",
        "+ Crane (Meaning 1 - Bird with a Long Neck):\n",
        "\n",
        "  + Sentence 1: A beautiful crane waded in the shallow water.\n",
        "  + Sentence 2: They used a crane to lift the heavy machinery onto the truck.\n",
        "+ Club (Meaning 1 - Social Organization):\n",
        "\n",
        "  + Sentence 1: I'm a member of the local chess club.\n",
        "  + Sentence 2: He used a golf club to hit the ball into the hole."
      ],
      "metadata": {
        "id": "E_rYv9ZJ3yLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent = input(\"Paste a sentence: \")\n",
        "amb = input(\"Type target word: \")"
      ],
      "metadata": {
        "id": "0dowCbih4Kk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.wsd import lesk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sentence = sent\n",
        "ambiguous = amb\n",
        "word_sense = lesk(word_tokenize(sentence), ambiguous)\n",
        "\n",
        "# Access the name of the disambiguated sense\n",
        "print(\"Disambiguated Sense:\", word_sense.name())\n",
        "# Access the definition of the disambiguated sense\n",
        "print(\"Sense Definition:\", word_sense.definition())\n"
      ],
      "metadata": {
        "id": "LyXk0_gx1_XH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.wsd import lesk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Define the sentence and ambiguous word\n",
        "sentence = \"He addressed the issue.\"\n",
        "ambiguous = \"address\"\n",
        "\n",
        "# Tokenize the sentence and perform POS tagging\n",
        "tokens = word_tokenize(sentence)\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "# Filter tokens based on the POS tag of the ambiguous word\n",
        "filtered_tokens = [token for token, pos in pos_tags if pos == 'POS_TAG_OF_AMBIGUOUS_WORD']\n",
        "\n",
        "# Perform Word Sense Disambiguation using Lesk algorithm\n",
        "word_sense = lesk(filtered_tokens, ambiguous)\n",
        "\n",
        "# Access the name of the disambiguated sense\n",
        "print(\"Disambiguated Sense:\", word_sense.name())\n",
        "# Access the definition of the disambiguated sense\n",
        "print(\"Sense Definition:\", word_sense.definition())\n"
      ],
      "metadata": {
        "id": "Q--8GuPz7Sry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "fRzJ9LU98inl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "from nltk.wsd import lesk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "sentence = \"Your addressed the issue clearly.\"\n",
        "ambiguous_word = \"addressed\"\n",
        "\n",
        "# Define a function to map Penn Treebank POS tags to WordNet POS tags\n",
        "def penn_to_wordnet_pos(penn_pos):\n",
        "    if penn_pos.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif penn_pos.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif penn_pos.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    elif penn_pos.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    else:\n",
        "        return None  # Return None for unknown POS tags\n",
        "\n",
        "# Define your sentence and ambiguous word\n",
        "sentence = \"The invalid is in the hospital.\"\n",
        "ambiguous_word = \"invalid\"\n",
        "\n",
        "# Tokenize the sentence and perform POS tagging\n",
        "tokens = word_tokenize(sentence)\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "# Determine the Penn Treebank POS tag for the ambiguous word\n",
        "ambiguous_word_pos_penn = None\n",
        "\n",
        "for token, pos in pos_tags:\n",
        "    if token == ambiguous_word:\n",
        "        ambiguous_word_pos_penn = pos\n",
        "        break\n",
        "\n",
        "# Map the Penn Treebank POS tag to WordNet POS tag\n",
        "ambiguous_word_pos_wordnet = penn_to_wordnet_pos(ambiguous_word_pos_penn)\n",
        "\n",
        "if ambiguous_word_pos_wordnet is None:\n",
        "    print(f\"Cannot determine WordNet POS category for '{ambiguous_word_pos_penn}'.\")\n",
        "else:\n",
        "    # Retrieve synsets and disambiguate sense\n",
        "    synsets = wordnet.synsets(ambiguous_word, pos=ambiguous_word_pos_wordnet)\n",
        "\n",
        "    if synsets:\n",
        "        word_sense = lesk(tokens, ambiguous_word, pos=ambiguous_word_pos_wordnet)\n",
        "        print(\"Disambiguated Sense:\", word_sense.name())\n",
        "        print(\"Sense Definition:\", word_sense.definition())\n",
        "    else:\n",
        "        print(f\"No synsets found for '{ambiguous_word}' in the '{ambiguous_word_pos_wordnet}' category.\")\n"
      ],
      "metadata": {
        "id": "NweKOui59FfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio"
      ],
      "metadata": {
        "id": "pIFHPo14-w39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "jpQshXAV-2UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Gradio app to display the ambiguous meaning (Not so reliable)\n",
        "import gradio as gr\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.wsd import lesk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Define a function to map Penn Treebank POS tags to WordNet POS tags\n",
        "def penn_to_wordnet_pos(penn_pos):\n",
        "    if penn_pos.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif penn_pos.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif penn_pos.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    elif penn_pos.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    else:\n",
        "        return None  # Return None for unknown POS tags\n",
        "\n",
        "# Define the disambiguation function that uses POS tagging\n",
        "def disambiguate_word_sense(sentence, ambiguous_word):\n",
        "    # Tokenize the sentence and perform POS tagging\n",
        "    tokens = word_tokenize(sentence)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "\n",
        "    # Find the POS tag for the ambiguous word in the tokenized sentence\n",
        "    ambiguous_word_pos_penn = None\n",
        "    for word, pos in pos_tags:\n",
        "        if word.lower() == ambiguous_word.lower():\n",
        "            ambiguous_word_pos_penn = pos\n",
        "            break\n",
        "\n",
        "    # If the POS tag is found, convert to WordNet POS tag\n",
        "    if ambiguous_word_pos_penn:\n",
        "        ambiguous_word_pos_wordnet = penn_to_wordnet_pos(ambiguous_word_pos_penn)\n",
        "    else:\n",
        "        return \"The ambiguous word was not found in the sentence.\"\n",
        "\n",
        "    if ambiguous_word_pos_wordnet:\n",
        "        # Perform Word Sense Disambiguation using Lesk algorithm\n",
        "        word_sense = lesk(tokens, ambiguous_word, pos=ambiguous_word_pos_wordnet)\n",
        "        if word_sense:\n",
        "            return f\"Disambiguated Sense: {word_sense.name()}\\nSense Definition: {word_sense.definition()}\"\n",
        "        else:\n",
        "            return f\"No disambiguated sense found for '{ambiguous_word}'.\"\n",
        "    else:\n",
        "        return f\"Cannot determine WordNet POS category for '{ambiguous_word}'.\"\n",
        "\n",
        "# Create the Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=disambiguate_word_sense,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=2, placeholder=\"Enter a sentence containing the ambiguous word\", label=\"Sentence\"),\n",
        "        gr.Textbox(placeholder=\"Enter the ambiguous word\", label=\"Ambiguous Word\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Result\"),\n",
        "    title=\"Word Sense Disambiguation\",\n",
        "    description=\"Enter a sentence and an ambiguous word to disambiguate its sense.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "o0LwagpzMBeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With POS (Just to get an idea)"
      ],
      "metadata": {
        "id": "8TtAnYv2Pq0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Gradio app with POS info to display the ambiguous meaning (Not so reliable)\n",
        "import gradio as gr\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.wsd import lesk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Ensure NLTK data is available\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # Open Multilingual Wordnet\n",
        "\n",
        "# Define the disambiguation function\n",
        "def disambiguate_word_sense(sentence, ambiguous_word, pos_choice):\n",
        "    # Map POS choice to WordNet POS\n",
        "    pos_map = {\n",
        "        \"Noun\": wordnet.NOUN,\n",
        "        \"Verb\": wordnet.VERB,\n",
        "        \"Adjective\": wordnet.ADJ,\n",
        "        \"Adverb\": wordnet.ADV\n",
        "    }\n",
        "\n",
        "    # Determine WordNet POS based on the user's choice\n",
        "    wordnet_pos = pos_map.get(pos_choice)\n",
        "\n",
        "    if wordnet_pos is None:\n",
        "        return f\"Cannot determine WordNet POS category for '{pos_choice}'.\"\n",
        "\n",
        "    tokens = word_tokenize(sentence)\n",
        "\n",
        "    # Use lesk to disambiguate the sense of the word\n",
        "    disambiguated_sense = lesk(tokens, ambiguous_word, pos=wordnet_pos)\n",
        "\n",
        "    if disambiguated_sense:\n",
        "        sense_name = disambiguated_sense.name()\n",
        "        sense_definition = disambiguated_sense.definition()  # Get the definition of the selected sense\n",
        "        return f\"Disambiguated Sense: {sense_name}\\nSense Definition: {sense_definition}\"\n",
        "    else:\n",
        "        return f\"No suitable sense found for '{ambiguous_word}' with POS '{pos_choice}'.\"\n",
        "\n",
        "# Create a Gradio interface with a submit button\n",
        "iface = gr.Interface(\n",
        "    fn=disambiguate_word_sense,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Sentence\", placeholder=\"Enter a sentence containing the ambiguous word\"),\n",
        "        gr.Textbox(label=\"Ambiguous Word\", placeholder=\"Enter the ambiguous word\"),\n",
        "        gr.Dropdown(label=\"Select POS\", choices=[\"Noun\", \"Verb\", \"Adjective\", \"Adverb\"])\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Result\"),\n",
        "    title=\"Word Sense Disambiguation\",\n",
        "    description=\"Enter a sentence, an ambiguous word, and select the part of speech (POS) of the word.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "R-3pFy6dK0z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [4] Bi-gram, N-gram, and Collocation"
      ],
      "metadata": {
        "id": "1UR_9g6EVrHC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ An \"ngram\" is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words, or base pairs according to the application.\n",
        "+ Ngrams are used in various applications like statistical language modeling, where they help predict the likelihood of a particular sequence of words.\n",
        "  + For example, in the sentence \"The quick brown fox jumps over the lazy dog,\" a 2-gram (or bigram) sequence would be (\"the quick\"), (\"quick brown\"), (\"brown fox\"), and so on.\n",
        "\n",
        "+ A \"concordance\" is a list of all occurrences of a particular search term in a corpus, presented together with a certain amount of context. This is often used in linguistic analysis to understand how words are used in different contexts."
      ],
      "metadata": {
        "id": "DLGLfP8vWxTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Bi-gram"
      ],
      "metadata": {
        "id": "NTn_44WjWjDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "ekyibE1QXEDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import bigrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample text\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Generate bigrams\n",
        "bigrams_list = list(bigrams(tokens))\n",
        "\n",
        "# Print the bigrams\n",
        "for bg in bigrams_list:\n",
        "    print(bg)\n"
      ],
      "metadata": {
        "id": "Jzq1KWAHWl1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ N-gram"
      ],
      "metadata": {
        "id": "Xp2XaVytXVIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Make sure to download the required NLTK models and data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Define a function to get n-grams from text\n",
        "def get_ngrams(text, n):\n",
        "    # Tokenize the text into words\n",
        "    tokens = word_tokenize(text)\n",
        "    # Generate n-grams\n",
        "    n_grams = ngrams(tokens, n)\n",
        "    # Convert to a list and return\n",
        "    return list(n_grams)\n",
        "\n",
        "# Sample text\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Get bigrams (2-grams)\n",
        "bigrams = get_ngrams(text, 2)\n",
        "print(\"Bigrams:\", bigrams)\n",
        "\n",
        "# Get trigrams (3-grams)\n",
        "trigrams = get_ngrams(text, 3)\n",
        "print(\"Trigrams:\", trigrams)\n",
        "\n",
        "# Get 4-grams\n",
        "fourgrams = get_ngrams(text, 4)\n",
        "print(\"4-grams:\", fourgrams)\n"
      ],
      "metadata": {
        "id": "glNxZEoTXUc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Collocation: Collocation in linguistics refers to the tendency of certain words to occur frequently together in a language. These word combinations often bear a meaning that is not entirely deducible from the individual words' meanings."
      ],
      "metadata": {
        "id": "L0pEAD8rXqDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "mdQYDcjNZ1Qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Python is an interpreted high-level general-purpose programming language. \\\n",
        "Python's design philosophy emphasizes code readability with its notable use of significant indentation. \\\n",
        "Its language constructs and object-oriented approach aim to help programmers write clear, \\\n",
        "logical code for small and large-scale projects. Python is dynamically-typed and garbage-collected. \\\n",
        "It supports multiple programming paradigms, including structured (particularly procedural), \\\n",
        "object-oriented, and functional programming. \\\n",
        "Python is often described as a \"batteries included\" language due to its comprehensive standard library. \\\n",
        "Python was created in the late 1980s as a successor to the ABC language. Python 2.0, released in 2000, \\\n",
        "introduced features like list comprehensions and a garbage collection system capable of collecting reference cycles. \\\n",
        "Python 3.0, released in 2008, was a major revision of the language that is not completely backward-compatible. \\\n",
        "The Python 2 series ended with version 2.7 in 2020. Python consistently ranks as one of the most popular programming languages.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "sDC18R9dajKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.collocations import TrigramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenizing the text\n",
        "tokens = word_tokenize(text.lower())  # Lowercasing for consistency\n",
        "\n",
        "# Removing stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words and word.isalnum()]\n",
        "\n",
        "# Finding bigram collocations\n",
        "bigram_measures = BigramAssocMeasures()\n",
        "finder = BigramCollocationFinder.from_words(filtered_tokens)\n",
        "finder.apply_freq_filter(2)  # Optional: filter out bigrams that occur less than 2 times\n",
        "\n",
        "# Display the 5 most frequent bigrams\n",
        "print(\"Top 5 bigram collocations:\")\n",
        "print(finder.nbest(bigram_measures.raw_freq, 5))\n",
        "\n",
        "# Finding trigram collocations\n",
        "trigram_measures = TrigramAssocMeasures()\n",
        "finder_tri = TrigramCollocationFinder.from_words(filtered_tokens)\n",
        "finder_tri.apply_freq_filter(2)  # Optional: filter out trigrams that occur less than 2 times\n",
        "\n",
        "# Display the 5 most frequent trigrams\n",
        "print(\"\\nTop 5 trigram collocations:\")\n",
        "print(finder_tri.nbest(trigram_measures.raw_freq, 5))\n"
      ],
      "metadata": {
        "id": "hVHvrne_aAXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [5] Concordance\n",
        "\n"
      ],
      "metadata": {
        "id": "XO6sk1zfZiGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import nltk\n",
        "# from nltk.tokenize import word_tokenize\n",
        "from nltk.text import Text\n",
        "\n",
        "# # Ensure that the necessary NLTK data is available\n",
        "# nltk.download('punkt')\n",
        "\n",
        "# Define a function to display concordances for a word in a given text\n",
        "def display_concordance(text, word, width=75, lines=25):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Create an NLTK text object\n",
        "    nltk_text = Text(tokens)\n",
        "    # Display concordances\n",
        "    nltk_text.concordance(word, width=width, lines=lines)\n",
        "\n",
        "\n",
        "# Display concordances for the word 'Python'\n",
        "display_concordance(text, 'Python')\n"
      ],
      "metadata": {
        "id": "y5P7DFWpXpvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "def display_concordance_custom(text, word, char_window=100):\n",
        "    sentences = sent_tokenize(text)\n",
        "    for sentence in sentences:\n",
        "        start_indices = [i for i in range(len(sentence)) if sentence.startswith(word, i)]\n",
        "        for start_index in start_indices:\n",
        "            start = max(0, start_index - char_window)\n",
        "            end = min(len(sentence), start_index + len(word) + char_window)\n",
        "            concordance = sentence[start:end].replace('\\n', ' ')\n",
        "            print(concordance)\n",
        "\n",
        "# Display concordances for the word 'Python'\n",
        "display_concordance_custom(text, 'Python')\n"
      ],
      "metadata": {
        "id": "yF3v4NaJYm23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A0I7eKXIV6Sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [6] Sentiment Analysis\n",
        "\n",
        "We will learn this in later weeks."
      ],
      "metadata": {
        "id": "CfbeycgdV6jt"
      }
    }
  ]
}