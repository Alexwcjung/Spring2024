{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MK316/Spring2024/blob/main/Corpus/TTR-and-lemmatization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŒ¿ Topics:\n",
        "\n",
        "## 1. **Type vs. Token**\n",
        "## 2. Lexical Diversity measures (10 types)"
      ],
      "metadata": {
        "id": "X1fv4aZ0B68x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1. Type vs. Token\n",
        "\n",
        "Example: A cat is chasing a mouse.\n",
        "\n",
        "+ Tokens: Tokens are often words, but they can also include punctuation, numbers, and other characters depending on the analysis. Simply put, tokens are the total number of words in a given text.\n",
        "\n",
        "  + 6 tokens in the given example\n",
        "\n",
        "+ Types: A type is the unique form of a token, disregarding its frequency of occurrence.\n",
        "\n",
        "  + 5 types in the given example."
      ],
      "metadata": {
        "id": "QuJHFaewBq5n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[text samples from Aesop fables](https://aesopsfables.org/)"
      ],
      "metadata": {
        "id": "QKFZ5TxMP5ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"An ant went to a Mistic-fountain.\"\n",
        "len(text)"
      ],
      "metadata": {
        "id": "cxO7aH8JQlXy",
        "outputId": "91743d60-c695-43fc-f762-3f5ab109b2fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **text.split()** # split string by space"
      ],
      "metadata": {
        "id": "1yjXhWTJQ6TX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step1 = text.split()\n",
        "print(step1)"
      ],
      "metadata": {
        "id": "1pc8WHWwQkrp",
        "outputId": "73639e09-5391-4b4d-9a82-f80ad81b0b2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['An', 'ant', 'went', 'to', 'a', 'Mistic-fountain.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "step2 = text.split(\".\") # delimiter here is '.'\n",
        "print(step2)"
      ],
      "metadata": {
        "id": "M_qkU6FwREHI",
        "outputId": "af460d79-3d28-43b8-809b-4d9afda28af8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['An ant went to a Mistic-fountain', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a longer text"
      ],
      "metadata": {
        "id": "UNpznL4oSIGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This includes all characters: letters, numbers, spaces, punctuation marks, and special characters.\n",
        "\n",
        "text = \"\"\"\n",
        "An ant went to a fountain to quench his thirst and, tumbling in, was almost drowned. But a dove that happened to be sitting on a neighboring tree saw the ant's danger and, plucking off a leaf, let it drop into the water before him. The ant mounting upon it, was presently wafted safely ashore.\n",
        "Just at that time, a fowler was spreading his net and was in the act of ensnaring the dove, when the ant, perceiving his object, bit his heel. The start this gave the man made him drop his net and the dove, aroused to a sense of her danger, flew safely away.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Number of strings: \", len(text))"
      ],
      "metadata": {
        "id": "nrIqszf5Pj1u",
        "outputId": "c12b1d2a-5069-4359-d8d9-10ac2fadbeee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of strings:  554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = text.split()\n",
        "len(tokens)"
      ],
      "metadata": {
        "id": "12i5q8r6QhcX",
        "outputId": "c2cee776-7667-46a1-e63a-c832cda330ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "107"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "types = set(tokens)\n",
        "len(types)\n"
      ],
      "metadata": {
        "id": "gVRl7ynYShZr",
        "outputId": "2b6864fd-82ce-4f77-c199-3034fdc1a511",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "76"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function"
      ],
      "metadata": {
        "id": "l0FsA9B5TPgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_types_and_tokens(text):\n",
        "    tokens = text.split()\n",
        "    types = set(tokens)\n",
        "    return len(types), len(tokens)"
      ],
      "metadata": {
        "id": "IVV1N3KjPuwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text\n",
        "\n",
        "num_types, num_tokens = count_types_and_tokens(text)\n",
        "print(\"Number of types:\", num_types)\n",
        "print(\"Number of tokens:\", num_tokens)"
      ],
      "metadata": {
        "id": "-06hNP2eTLdG",
        "outputId": "417d4d38-97d1-4b7b-f492-7381d1d0fc13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of types: 76\n",
            "Number of tokens: 107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization\n",
        "\n",
        "+ lemma: a dictionary form or base form of a set of words.\n",
        "+ example: 'run, runs, running, ran' => 'run'\n",
        "\n",
        "We will use {nltk} modules"
      ],
      "metadata": {
        "id": "NQXlTrHOTYBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "BkZIe6KlTzdi",
        "outputId": "522c6cef-1a64-436a-e9d4-3bc2653af40e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Download the WordNet resource (if not already downloaded)\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "E8HhAfXZT_Xt",
        "outputId": "65b762c2-2381-41dd-bb91-d9da86bdd403",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The function below, get_wordnet_pos, is designed to map the part-of-speech (POS) tags provided by NLTK's pos_tag function to the format that is recognized by the WordNet Lemmatizer, which is part of the NLTK library. This mapping is essential for accurate lemmatization, as it allows the lemmatizer to understand the grammatical category of each word."
      ],
      "metadata": {
        "id": "pBsUOddJWRoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define get_wordnet_pos(word)\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ],
      "metadata": {
        "id": "asSvbbChUMf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"The cats are running faster than the dogs\""
      ],
      "metadata": {
        "id": "65oy83YqUriA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the WordNet lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "# Lemmatization using POS tags\n",
        "lemmatized_output = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]\n",
        "\n",
        "print('Original Sentence:', sentence)\n",
        "print('Lemmatized Sentence:', ' '.join(lemmatized_output))"
      ],
      "metadata": {
        "id": "JlfD53I4UhpX",
        "outputId": "f7facae0-700e-47e6-fb89-dcb702c5efa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: The cats are running faster than the dogs\n",
            "Lemmatized Sentence: The cat be run faster than the dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization practice with our text"
      ],
      "metadata": {
        "id": "Hrc_6o_DU_MB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "An ant went to a fountain to quench his thirst and, tumbling in, was almost drowned. But a dove that happened to be sitting on a neighboring tree saw the ant's danger and, plucking off a leaf, let it drop into the water before him. The ant mounting upon it, was presently wafted safely ashore.\n",
        "Just at that time, a fowler was spreading his net and was in the act of ensnaring the dove, when the ant, perceiving his object, bit his heel. The start this gave the man made him drop his net and the dove, aroused to a sense of her danger, flew safely away.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "kvWYRRrOVFpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the WordNet lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Lemmatization using POS tags\n",
        "lemmatized_output = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]\n",
        "\n",
        "print('Original Sentence:', text)\n",
        "print('Lemmatized Sentence:', ' '.join(lemmatized_output))"
      ],
      "metadata": {
        "id": "mnFhi93IVMPg",
        "outputId": "07a187f0-8826-4d74-ec53-475c7c45754f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: \n",
            "An ant went to a fountain to quench his thirst and, tumbling in, was almost drowned. But a dove that happened to be sitting on a neighboring tree saw the ant's danger and, plucking off a leaf, let it drop into the water before him. The ant mounting upon it, was presently wafted safely ashore.\n",
            "Just at that time, a fowler was spreading his net and was in the act of ensnaring the dove, when the ant, perceiving his object, bit his heel. The start this gave the man made him drop his net and the dove, aroused to a sense of her danger, flew safely away.\n",
            "\n",
            "Lemmatized Sentence: An ant go to a fountain to quench his thirst and , tumble in , be almost drown . But a dove that happen to be sit on a neighbor tree saw the ant 's danger and , pluck off a leaf , let it drop into the water before him . The ant mount upon it , be presently waft safely ashore . Just at that time , a fowler be spread his net and be in the act of ensnare the dove , when the ant , perceive his object , bit his heel . The start this give the man make him drop his net and the dove , arouse to a sense of her danger , flew safely away .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(lemmatized_output)"
      ],
      "metadata": {
        "id": "1IcWfn3LXX3C",
        "outputId": "fcad03f5-2f6e-4acc-dc93-d83237924266",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare text tokens, types, and lemmatized"
      ],
      "metadata": {
        "id": "q4L-fuHZXd1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "An ant went to a fountain to quench his thirst and, tumbling in, was almost drowned. But a dove that happened to be sitting on a neighboring tree saw the ant's danger and, plucking off a leaf, let it drop into the water before him. The ant mounting upon it, was presently wafted safely ashore.\n",
        "Just at that time, a fowler was spreading his net and was in the act of ensnaring the dove, when the ant, perceiving his object, bit his heel. The start this gave the man made him drop his net and the dove, aroused to a sense of her danger, flew safely away.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_E_UomUKYeLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = text.split(); print(len(tokens))\n",
        "print(len(lemmatized_output))"
      ],
      "metadata": {
        "id": "SqLZXVAXYiJa",
        "outputId": "8c6b38a2-476a-4ee2-abba-9890215f9ceb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "107\n",
            "124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Types in the text order"
      ],
      "metadata": {
        "id": "H5u8BYYbagUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'tokens' is already defined\n",
        "\n",
        "types_in_order = []\n",
        "seen = set()\n",
        "\n",
        "for token in tokens:\n",
        "    if token not in seen:\n",
        "        seen.add(token)\n",
        "        types_in_order.append(token)\n",
        "\n",
        "# Now 'types_in_order' contains unique elements from 'tokens' in the order they appear in the text\n"
      ],
      "metadata": {
        "id": "upt-2KK1aVpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dataframe with tokens, types_in_order, lemmatized_output\n",
        "\n",
        "print(len(tokens))\n",
        "print(len(types_in_order))\n",
        "print(len(lemmatized_output))"
      ],
      "metadata": {
        "id": "pYqm_ezLZtzP",
        "outputId": "22c47acb-ae63-4004-aa5c-e9d04e28990b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "107\n",
            "76\n",
            "124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "id": "Wqo2YtsBYIm-",
        "outputId": "6f691b66-0423-45d6-9b90-122000719ebb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming tokens, types_in_order, and lemmatized_output are already defined\n",
        "# and their lengths are 107, 76, 124 respectively\n",
        "\n",
        "# Extend types_in_order and tokens with 'None' to match the length of lemmatized_output\n",
        "types_in_order.extend([None] * (len(lemmatized_output) - len(types_in_order)))\n",
        "tokens.extend([None] * (len(lemmatized_output) - len(tokens)))\n",
        "\n",
        "# Create the DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'Tokens': tokens,\n",
        "    'Types': types_in_order,\n",
        "    'Lemmatized Output': lemmatized_output\n",
        "})\n",
        "\n",
        "df[1:20]\n"
      ],
      "metadata": {
        "id": "nlzdg1l4YMu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TTR (Type-to-Token Ratio)"
      ],
      "metadata": {
        "id": "RRvfPCr7VEV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have already calculated the number of types and tokens\n",
        "number_of_types = len(types)  # Number of unique words\n",
        "number_of_tokens = len(tokens)  # Total number of words\n",
        "\n",
        "# Calculate TTR\n",
        "TTR = number_of_types / number_of_tokens\n",
        "\n",
        "print(\"Type-Token Ratio (TTR):\", TTR)\n"
      ],
      "metadata": {
        "id": "eb9oBqo9cmNi",
        "outputId": "61060666-fb05-4a8a-fb20-827e35f5b128",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type-Token Ratio (TTR): 0.6129032258064516\n"
          ]
        }
      ]
    }
  ]
}