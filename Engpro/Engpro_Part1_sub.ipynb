{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM31QT2ufn0gMcvGsAJ2GmY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MK316/Spring2024/blob/main/Engpro/Engpro_Part1_sub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŒ± **Part 1. Supplementary**\n",
        "\n",
        "+ What is sound?\n",
        "+ Dialectal vowel variation\n",
        "+ Listening and speaking"
      ],
      "metadata": {
        "id": "w6iykHYpdMX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install gTTS\n",
        "from gtts import gTTS\n",
        "from IPython.display import YouTubeVideo, display, Audio, Image\n",
        "#@markdown ðŸ”„ Install {pydub} {gtts}\n",
        "%%capture\n",
        "!pip install pydub"
      ],
      "metadata": {
        "id": "h_mrClnPK0kl",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mydkqYLxHh3C",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ðŸ”„ Making functions: etts('text'), ktts('text')\n",
        "\n",
        "\n",
        "def etts(text):\n",
        "  text_to_say = text\n",
        "\n",
        "  gtts_object = gTTS(text = text_to_say,\n",
        "                     lang = \"en\", tld = \"us\",\n",
        "                    slow = False)\n",
        "\n",
        "  gtts_object.save(\"E-audio.mp3\")\n",
        "  return Audio(\"E-audio.mp3\")\n",
        "\n",
        "def ktts(text):\n",
        "  text_to_say = text\n",
        "\n",
        "  gtts_object = gTTS(text = text_to_say,\n",
        "                     lang = \"ko\",\n",
        "                    slow = False)\n",
        "\n",
        "  gtts_object.save(\"K-audio.mp3\")\n",
        "  return Audio(\"K-audio.mp3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **[1] What is sound?**\n",
        "\n",
        "+ Sound to the brain\n",
        "+ Pysical aspect (acoustics)\n",
        "+ Pysiological aspect"
      ],
      "metadata": {
        "id": "B0crfy-7eoVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Generating sound (digital signal)"
      ],
      "metadata": {
        "id": "832pbg-5hS0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ðŸ”„ (2/4)Generate a sample speech (File: E-audio.mp3):\n",
        "\n",
        "txt = input(\"Type text: \")\n",
        "etts(txt)\n",
        "Audio(\"E-audio.mp3\", autoplay=True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "G8w6sROqfFgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ðŸ”„ (3/4) Specify file names to convert: ðŸ“Œ E-audio.mp3 => save as \"sample.wav\"\n",
        "!pip install pydub\n",
        "from pydub import AudioSegment\n",
        "\n",
        "inaudio = \"E-audio.mp3\"\n",
        "outaudio = \"sample.wav\"\n",
        "sound = AudioSegment.from_mp3(inaudio)\n",
        "sound.export(outaudio, format=\"wav\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "T3vCaf4SfWvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ðŸ”„ (4/4) Display waveform:\n",
        "import scipy.io.wavfile as wavfile\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "audio = AudioSegment.from_file(outaudio, format=\"wav\")\n",
        "print(\"Sapling rate: \", audio.frame_rate)\n",
        "print(f\"Text: {txt}\")\n",
        "fs, data = wavfile.read(outaudio) # replace with the name of your audio file\n",
        "plt.figure(figsize=(18,4))\n",
        "plt.xlabel('time (samples)')\n",
        "plt.plot(data)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "aM_Srs2RfZ-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@markdown ðŸ”„ Zoom in the waveform (0.52~0.8 seconds = 12000 ~ 19680 samples)\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import wavfile\n",
        "\n",
        "# Read the waveform data\n",
        "rate, data = wavfile.read('/content/sample.wav')\n",
        "\n",
        "# Plot the waveform\n",
        "fig, ax = plt.subplots(figsize = (18,3))\n",
        "\n",
        "ax.plot(data)\n",
        "\n",
        "st = input(\"Start time (sec.) e.g., 0.52: \")\n",
        "et = input(\"End time(sec.): e.g., 0.8\")\n",
        "# Set the x-axis limits to zoom in on a specific time period\n",
        "\n",
        "stsmp = (24000*(float(st)*1000))/1000\n",
        "etsmp = (24000*(float(et)*1000))/1000\n",
        "print(\"Duration in samples: from {} to {}\".format(stsmp,etsmp))\n",
        "\n",
        "start_time = float(st) # start time in seconds\n",
        "end_time = float(et) # end time in seconds\n",
        "ax.set_xlim(start_time * rate, end_time * rate)\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fjexrDJ6fcR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B. Travel of sound to the brain"
      ],
      "metadata": {
        "id": "7_NKv87xhb3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ðŸŽ¬ Journey of sound to the brain 2m 26s\n",
        "from IPython.display import YouTubeVideo, display\n",
        "video = YouTubeVideo(\"eQEaiZ2j9oc\", width=600, height=\"400\")\n",
        "display(video)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "c3p3ORUpe0hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **[2] Dialectal vowel variation**"
      ],
      "metadata": {
        "id": "7ucLgS0IelLf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Dc80jg0EdBXk"
      },
      "outputs": [],
      "source": [
        "#@markdown ðŸŽ§ TTS with dialectal variation. (Not so reliable.)\n",
        "\n",
        "variety = \"en-co.in\" #@param = [\"en-us\",\"en-co.uk\",\"en-ca\",\"en-com.au\",\"en-ie\",\"en-co.in\"]\n",
        "v1 = variety.split('-')\n",
        "language = v1[0]\n",
        "dialect = v1[1]\n",
        "\n",
        "#@markdown Sample text: I'm thirty and my wife is thirty too.\n",
        "text_to_say = input(\"Type text: \")\n",
        "gtts_object = gTTS(text = text_to_say,\n",
        "                   lang = language, tld = dialect,\n",
        "                   slow = False)\n",
        "\n",
        "gtts_object.save(\"E-audio.mp3\")\n",
        "Audio(\"E-audio.mp3\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ktts(txt)\n",
        "Audio(\"K-audio.mp3\", autoplay=True)"
      ],
      "metadata": {
        "id": "c6JzgD3Zh4ZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ’¦ [Application](https://mrkim21.github.io/appfolder/foreignaccent.html): Foreign accents"
      ],
      "metadata": {
        "id": "4pJUV0iNiBVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **[3] What can be a better approach to improve listening and speaking other than 'listen-and-repeat'**"
      ],
      "metadata": {
        "id": "M2P7MGNeb_5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The critical period hypothesis** was first proposed by Montreal neurologist Wilder Penfield and co-author Lamar Roberts in their 1959 book Speech and Brain Mechanisms, and was popularized by Eric Lenneberg in 1967 with Biological Foundations of Language.\n",
        "\n",
        "+ Penfield, W.; Roberts, L. (1959). _Speech and Brain Mechanisms._ Princeton: Princeton University Press. ISBN 978-0-691-08039-0.\n",
        "+ Lenneberg, E.H. (1967). _Biological Foundations of Language._ New York: Wiley. ISBN 978-0-89874-700-3."
      ],
      "metadata": {
        "id": "UumGJsPDn2vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ðŸŽ¬ Learning English by listening (Pink Panther Hamburger scene) 2m 2s\n",
        "from IPython.display import YouTubeVideo, display\n",
        "video = YouTubeVideo(\"lz0IT4Uk2xQ\", width=600, height=\"400\")\n",
        "display(video)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Vvq2n7Haj_II"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Critical Period Hypothesis\n",
        "from IPython.display import display, Image\n",
        "url = \"https://github.com/MK316/images/raw/main/CPH.png\"\n",
        "Image(url, width =\" 600\", height = \"500\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hZOEVDj9b_Xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top-down process for adult learners\n",
        "\n",
        "1. Understand articulatory process\n",
        "2. Learn the gestures of speech articulation\n",
        "3. Connect how you make sounds to what you hear, and vice versa.\n",
        "4. Increase inputs and outputs"
      ],
      "metadata": {
        "id": "l9OgjChdkkvf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [â›„ Activity] Q & As: [Good teacher qualification](https://github.com/MK316/Spring2024/blob/main/Engpro/data/goodteacher.md)"
      ],
      "metadata": {
        "id": "S7QgNZOynIjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Quote: best teacher\n",
        "url = \"https://github.com/MK316/images/raw/main/quote_looking.png\"\n",
        "Image(url, width = \"800\", height =\" 300\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1AQkUNb9kgcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Question:\n",
        "\n",
        "text = \"What qualities are essential to effectively educate and inspire future generations?\"\n",
        "\n",
        "etts(text)\n",
        "Audio(\"E-audio.mp3\")"
      ],
      "metadata": {
        "id": "JTaSGE_FnrP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Answer (with Korean accent):\n",
        "\n",
        "text = \"In an increasingly digital world, a good teacher must be technologically savvy, not just in using educational technology, but in integrating it meaningfully into lessons to enhance learning. They should prepare students to be digitally literate, critical thinkers in an online world.\"\n",
        "\n",
        "ktts(text)\n",
        "Audio(\"K-audio.mp3\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RKxpxkRqpMYA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}